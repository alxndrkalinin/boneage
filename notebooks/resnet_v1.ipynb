{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary(input_size, model):\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = '%s-%i' % (class_name, module_idx+1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key]['input_shape'] = list(input[0].size())\n",
    "            summary[m_key]['input_shape'][0] = -1\n",
    "            summary[m_key]['output_shape'] = list(output.size())\n",
    "            summary[m_key]['output_shape'][0] = -1\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, 'weight'):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                if module.weight.requires_grad:\n",
    "                    summary[m_key]['trainable'] = True\n",
    "                else:\n",
    "                    summary[m_key]['trainable'] = False\n",
    "            if hasattr(module, 'bias'):\n",
    "                params +=  torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key]['nb_params'] = params\n",
    "\n",
    "        if not isinstance(module, nn.Sequential) and \\\n",
    "           not isinstance(module, nn.ModuleList) and \\\n",
    "           not (module == model):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # check if there are multiple inputs to the network\n",
    "    if isinstance(input_size[0], (list, tuple)):\n",
    "        x = [Variable(torch.rand(1,*in_size)) for in_size in input_size]\n",
    "    else:\n",
    "        x = Variable(torch.rand(1,*input_size))\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "    # make a forward pass\n",
    "    model(x)\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../data/boneage-training-dataset/'\n",
    "TRAIN_LABEL = '../data/train.csv'\n",
    "FILETYPE = '.png'\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeDataset(Dataset):\n",
    "    \"\"\"Bone Age dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, labels, filetype, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filetype = filetype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.labels.iloc[idx]['id']) + self.filetype)\n",
    "        img = Image.open(img_name)\n",
    "#         image = cv2.imread(img_name)[:,:,::-1]\n",
    "        label = self.labels.iloc[idx]['boneage'].astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        sample = {'image': img, 'label': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(1024),\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(1024),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(TRAIN_LABEL)\n",
    "bins = xrange(20)\n",
    "labels['bins'] = np.digitize(labels['boneage'], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/software/miniconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=4)\n",
    "splits = []\n",
    "for train, test in skf.split(np.zeros(len(labels)), labels['bins']):\n",
    "    fold_label = {\n",
    "        'train': labels.loc[train, ('id', 'boneage')],\n",
    "        'val': labels.loc[test, ('id', 'boneage')]\n",
    "    }\n",
    "    splits.append(fold_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_label = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: BoneAgeDataset(TRAIN_DIR, fold_label[x], FILETYPE, data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=16)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample in enumerate(dataloders['train']):\n",
    "    print(i_batch, sample['image'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Conv2d-1', OrderedDict([('input_shape', [-1, 1L, 224L, 224L]), ('output_shape', [-1, 10L, 220L, 220L]), ('trainable', True), ('nb_params', 260)])), ('Conv2d-2', OrderedDict([('input_shape', [-1, 10L, 110L, 110L]), ('output_shape', [-1, 20L, 106L, 106L]), ('trainable', True), ('nb_params', 5020)])), ('Dropout2d-3', OrderedDict([('input_shape', [-1, 20L, 106L, 106L]), ('output_shape', [-1, 20L, 106L, 106L]), ('nb_params', 0)])), ('Linear-4', OrderedDict([('input_shape', [-1, 56180L]), ('output_shape', [-1, 75L]), ('trainable', True), ('nb_params', 4213575)])), ('Linear-5', OrderedDict([('input_shape', [-1, 75L]), ('output_shape', [-1, 1L]), ('trainable', True), ('nb_params', 76)]))])\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 28, 28)):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        n_size = self._get_conv_output(input_shape)\n",
    "        self.fc1 = nn.Linear(n_size, 75)\n",
    "        self.fc2 = nn.Linear(75, 1)\n",
    "        \n",
    "        # generate input sample and forward to get shape\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = Variable(torch.rand(bs, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net([1, 224, 224])\n",
    "    \n",
    "print(summary([1, 224, 224], model))\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccc(x, y):\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "    xm = x.sub(mean_x)\n",
    "    ym = y.sub(mean_y)\n",
    "    xn = torch.norm(xm, 2)\n",
    "    yn = torch.norm(ym, 2)\n",
    "    xydot = ym.dot(xm.view(-1))\n",
    "\n",
    "    ccc_num = 2 * xydot / xm.size()[0]\n",
    "    ccc_den = xn + yn + torch.pow(mean_x.sub(mean_y), 2)\n",
    "\n",
    "    ccc = ccc_num / ccc_den\n",
    "    return ccc\n",
    "    \n",
    "\n",
    "def train(epoch, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        data = sample['image']\n",
    "        target = sample['label'].float()\n",
    "        if use_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            c = ccc(output, target)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tCCC: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0], c.data.cpu().numpy()[0]))\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for sample in test_loader:\n",
    "        data = sample['image']\n",
    "        target = sample['label'].float()\n",
    "        if use_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.l1_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        c = ccc(output, target)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, CCC: {}\\n'.format(test_loss, c.data.cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/9453 (0%)]\tLoss: 22.471260\tCCC: 0.0000\n",
      "Train Epoch: 1 [800/9453 (8%)]\tLoss: 42.518112\tCCC: 0.0000\n",
      "Train Epoch: 1 [1600/9453 (17%)]\tLoss: 64.403717\tCCC: 0.0000\n",
      "Train Epoch: 1 [2400/9453 (25%)]\tLoss: 52.500000\tCCC: 0.0000\n",
      "Train Epoch: 1 [3200/9453 (34%)]\tLoss: 7.521950\tCCC: 0.0000\n",
      "Train Epoch: 1 [4000/9453 (42%)]\tLoss: 42.424049\tCCC: 0.0000\n",
      "Train Epoch: 1 [4800/9453 (51%)]\tLoss: 36.051239\tCCC: 0.0000\n",
      "Train Epoch: 1 [5600/9453 (59%)]\tLoss: 38.000000\tCCC: 0.0000\n",
      "Train Epoch: 1 [6400/9453 (68%)]\tLoss: 30.695778\tCCC: 0.0000\n",
      "Train Epoch: 1 [7200/9453 (76%)]\tLoss: 9.229774\tCCC: 0.0000\n",
      "Train Epoch: 1 [8000/9453 (85%)]\tLoss: 25.500000\tCCC: 0.0000\n",
      "Train Epoch: 1 [8800/9453 (93%)]\tLoss: 19.644867\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3795, CCC: 0.0\n",
      "\n",
      "Train Epoch: 2 [0/9453 (0%)]\tLoss: 25.458328\tCCC: 0.0000\n",
      "Train Epoch: 2 [800/9453 (8%)]\tLoss: 36.000000\tCCC: 0.0000\n",
      "Train Epoch: 2 [1600/9453 (17%)]\tLoss: 27.512268\tCCC: 0.0000\n",
      "Train Epoch: 2 [2400/9453 (25%)]\tLoss: 33.689606\tCCC: 0.0000\n",
      "Train Epoch: 2 [3200/9453 (34%)]\tLoss: 30.000000\tCCC: 0.0000\n",
      "Train Epoch: 2 [4000/9453 (42%)]\tLoss: 15.500000\tCCC: 0.0000\n",
      "Train Epoch: 2 [4800/9453 (51%)]\tLoss: 29.506447\tCCC: 0.0000\n",
      "Train Epoch: 2 [5600/9453 (59%)]\tLoss: 20.245522\tCCC: 0.0000\n",
      "Train Epoch: 2 [6400/9453 (68%)]\tLoss: 52.357132\tCCC: 0.0000\n",
      "Train Epoch: 2 [7200/9453 (76%)]\tLoss: 24.000000\tCCC: 0.0000\n",
      "Train Epoch: 2 [8000/9453 (85%)]\tLoss: 35.250000\tCCC: 0.0000\n",
      "Train Epoch: 2 [8800/9453 (93%)]\tLoss: 14.251060\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3771, CCC: 0.0\n",
      "\n",
      "Train Epoch: 3 [0/9453 (0%)]\tLoss: 32.250000\tCCC: 0.0000\n",
      "Train Epoch: 3 [800/9453 (8%)]\tLoss: 25.504944\tCCC: 0.0000\n",
      "Train Epoch: 3 [1600/9453 (17%)]\tLoss: 34.500000\tCCC: 0.0000\n",
      "Train Epoch: 3 [2400/9453 (25%)]\tLoss: 32.719826\tCCC: 0.0000\n",
      "Train Epoch: 3 [3200/9453 (34%)]\tLoss: 24.015579\tCCC: 0.0000\n",
      "Train Epoch: 3 [4000/9453 (42%)]\tLoss: 47.651222\tCCC: 0.0000\n",
      "Train Epoch: 3 [4800/9453 (51%)]\tLoss: 31.596100\tCCC: 0.0000\n",
      "Train Epoch: 3 [5600/9453 (59%)]\tLoss: 24.756653\tCCC: 0.0000\n",
      "Train Epoch: 3 [6400/9453 (68%)]\tLoss: 40.022316\tCCC: 0.0000\n",
      "Train Epoch: 3 [7200/9453 (76%)]\tLoss: 37.987175\tCCC: 0.0000\n",
      "Train Epoch: 3 [8000/9453 (85%)]\tLoss: 21.500000\tCCC: 0.0000\n",
      "Train Epoch: 3 [8800/9453 (93%)]\tLoss: 38.300392\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.4796, CCC: 0.0\n",
      "\n",
      "Train Epoch: 4 [0/9453 (0%)]\tLoss: 48.000000\tCCC: 0.0000\n",
      "Train Epoch: 4 [800/9453 (8%)]\tLoss: 58.154457\tCCC: 0.0000\n",
      "Train Epoch: 4 [1600/9453 (17%)]\tLoss: 49.000000\tCCC: 0.0000\n",
      "Train Epoch: 4 [2400/9453 (25%)]\tLoss: 26.000000\tCCC: 0.0000\n",
      "Train Epoch: 4 [3200/9453 (34%)]\tLoss: 55.500000\tCCC: 0.0000\n",
      "Train Epoch: 4 [4000/9453 (42%)]\tLoss: 65.500000\tCCC: 0.0000\n",
      "Train Epoch: 4 [4800/9453 (51%)]\tLoss: 16.500000\tCCC: 0.0000\n",
      "Train Epoch: 4 [5600/9453 (59%)]\tLoss: 43.005959\tCCC: 0.0000\n",
      "Train Epoch: 4 [6400/9453 (68%)]\tLoss: 43.943985\tCCC: 0.0000\n",
      "Train Epoch: 4 [7200/9453 (76%)]\tLoss: 36.503510\tCCC: 0.0000\n",
      "Train Epoch: 4 [8000/9453 (85%)]\tLoss: 10.500000\tCCC: 0.0000\n",
      "Train Epoch: 4 [8800/9453 (93%)]\tLoss: 29.041954\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.4194, CCC: 0.0\n",
      "\n",
      "Train Epoch: 5 [0/9453 (0%)]\tLoss: 27.086227\tCCC: 0.0000\n",
      "Train Epoch: 5 [800/9453 (8%)]\tLoss: 32.972092\tCCC: 0.0000\n",
      "Train Epoch: 5 [1600/9453 (17%)]\tLoss: 23.000000\tCCC: 0.0000\n",
      "Train Epoch: 5 [2400/9453 (25%)]\tLoss: 51.500000\tCCC: 0.0000\n",
      "Train Epoch: 5 [3200/9453 (34%)]\tLoss: 25.510254\tCCC: 0.0000\n",
      "Train Epoch: 5 [4000/9453 (42%)]\tLoss: 56.500000\tCCC: 0.0000\n",
      "Train Epoch: 5 [4800/9453 (51%)]\tLoss: 10.518654\tCCC: 0.0000\n",
      "Train Epoch: 5 [5600/9453 (59%)]\tLoss: 64.323219\tCCC: 0.0000\n",
      "Train Epoch: 5 [6400/9453 (68%)]\tLoss: 15.016129\tCCC: 0.0000\n",
      "Train Epoch: 5 [7200/9453 (76%)]\tLoss: 51.000000\tCCC: 0.0000\n",
      "Train Epoch: 5 [8000/9453 (85%)]\tLoss: 30.000000\tCCC: 0.0000\n",
      "Train Epoch: 5 [8800/9453 (93%)]\tLoss: 33.500000\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3765, CCC: 0.0\n",
      "\n",
      "Train Epoch: 6 [0/9453 (0%)]\tLoss: 18.500000\tCCC: 0.0000\n",
      "Train Epoch: 6 [800/9453 (8%)]\tLoss: 7.500000\tCCC: 0.0000\n",
      "Train Epoch: 6 [1600/9453 (17%)]\tLoss: 15.500000\tCCC: 0.0000\n",
      "Train Epoch: 6 [2400/9453 (25%)]\tLoss: 29.794983\tCCC: 0.0000\n",
      "Train Epoch: 6 [3200/9453 (34%)]\tLoss: 18.000000\tCCC: 0.0000\n",
      "Train Epoch: 6 [4000/9453 (42%)]\tLoss: 24.537247\tCCC: 0.0000\n",
      "Train Epoch: 6 [4800/9453 (51%)]\tLoss: 33.500000\tCCC: 0.0000\n",
      "Train Epoch: 6 [5600/9453 (59%)]\tLoss: 29.000000\tCCC: 0.0000\n",
      "Train Epoch: 6 [6400/9453 (68%)]\tLoss: 35.799599\tCCC: 0.0000\n",
      "Train Epoch: 6 [7200/9453 (76%)]\tLoss: 24.000000\tCCC: 0.0000\n",
      "Train Epoch: 6 [8000/9453 (85%)]\tLoss: 46.492775\tCCC: 0.0000\n",
      "Train Epoch: 6 [8800/9453 (93%)]\tLoss: 39.474396\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3754, CCC: 0.0\n",
      "\n",
      "Train Epoch: 7 [0/9453 (0%)]\tLoss: 41.250000\tCCC: 0.0000\n",
      "Train Epoch: 7 [800/9453 (8%)]\tLoss: 24.699341\tCCC: 0.0000\n",
      "Train Epoch: 7 [1600/9453 (17%)]\tLoss: 15.500000\tCCC: 0.0000\n",
      "Train Epoch: 7 [2400/9453 (25%)]\tLoss: 30.000000\tCCC: 0.0000\n",
      "Train Epoch: 7 [3200/9453 (34%)]\tLoss: 50.443642\tCCC: 0.0000\n",
      "Train Epoch: 7 [4000/9453 (42%)]\tLoss: 41.451050\tCCC: 0.0000\n",
      "Train Epoch: 7 [4800/9453 (51%)]\tLoss: 18.016975\tCCC: 0.0000\n",
      "Train Epoch: 7 [5600/9453 (59%)]\tLoss: 16.517601\tCCC: 0.0000\n",
      "Train Epoch: 7 [6400/9453 (68%)]\tLoss: 43.966339\tCCC: 0.0000\n",
      "Train Epoch: 7 [7200/9453 (76%)]\tLoss: 37.386681\tCCC: 0.0000\n",
      "Train Epoch: 7 [8000/9453 (85%)]\tLoss: 18.559601\tCCC: 0.0000\n",
      "Train Epoch: 7 [8800/9453 (93%)]\tLoss: 22.044968\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.4312, CCC: 0.0\n",
      "\n",
      "Train Epoch: 8 [0/9453 (0%)]\tLoss: 30.386589\tCCC: 0.0000\n",
      "Train Epoch: 8 [800/9453 (8%)]\tLoss: 34.536118\tCCC: 0.0000\n",
      "Train Epoch: 8 [1600/9453 (17%)]\tLoss: 36.500000\tCCC: 0.0000\n",
      "Train Epoch: 8 [2400/9453 (25%)]\tLoss: 31.502800\tCCC: 0.0000\n",
      "Train Epoch: 8 [3200/9453 (34%)]\tLoss: 24.500000\tCCC: 0.0000\n",
      "Train Epoch: 8 [4000/9453 (42%)]\tLoss: 19.668091\tCCC: 0.0000\n",
      "Train Epoch: 8 [4800/9453 (51%)]\tLoss: 18.500000\tCCC: 0.0000\n",
      "Train Epoch: 8 [5600/9453 (59%)]\tLoss: 40.500000\tCCC: 0.0000\n",
      "Train Epoch: 8 [6400/9453 (68%)]\tLoss: 19.092308\tCCC: 0.0000\n",
      "Train Epoch: 8 [7200/9453 (76%)]\tLoss: 47.969131\tCCC: 0.0000\n",
      "Train Epoch: 8 [8000/9453 (85%)]\tLoss: 61.208298\tCCC: 0.0000\n",
      "Train Epoch: 8 [8800/9453 (93%)]\tLoss: 40.988602\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3759, CCC: 0.0\n",
      "\n",
      "Train Epoch: 9 [0/9453 (0%)]\tLoss: 23.000000\tCCC: 0.0000\n",
      "Train Epoch: 9 [800/9453 (8%)]\tLoss: 27.000000\tCCC: 0.0000\n",
      "Train Epoch: 9 [1600/9453 (17%)]\tLoss: 39.739342\tCCC: 0.0000\n",
      "Train Epoch: 9 [2400/9453 (25%)]\tLoss: 37.637253\tCCC: 0.0000\n",
      "Train Epoch: 9 [3200/9453 (34%)]\tLoss: 52.612572\tCCC: 0.0000\n",
      "Train Epoch: 9 [4000/9453 (42%)]\tLoss: 24.036530\tCCC: 0.0000\n",
      "Train Epoch: 9 [4800/9453 (51%)]\tLoss: 32.204659\tCCC: 0.0000\n",
      "Train Epoch: 9 [5600/9453 (59%)]\tLoss: 28.498787\tCCC: 0.0000\n",
      "Train Epoch: 9 [6400/9453 (68%)]\tLoss: 22.046501\tCCC: 0.0000\n",
      "Train Epoch: 9 [7200/9453 (76%)]\tLoss: 38.261177\tCCC: 0.0000\n",
      "Train Epoch: 9 [8000/9453 (85%)]\tLoss: 26.000000\tCCC: 0.0000\n",
      "Train Epoch: 9 [8800/9453 (93%)]\tLoss: 21.748650\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3766, CCC: 0.0\n",
      "\n",
      "Train Epoch: 10 [0/9453 (0%)]\tLoss: 48.707260\tCCC: 0.0000\n",
      "Train Epoch: 10 [800/9453 (8%)]\tLoss: 24.000000\tCCC: 0.0000\n",
      "Train Epoch: 10 [1600/9453 (17%)]\tLoss: 41.473862\tCCC: 0.0000\n",
      "Train Epoch: 10 [2400/9453 (25%)]\tLoss: 33.032684\tCCC: 0.0000\n",
      "Train Epoch: 10 [3200/9453 (34%)]\tLoss: 4.537971\tCCC: 0.0000\n",
      "Train Epoch: 10 [4000/9453 (42%)]\tLoss: 27.000000\tCCC: 0.0000\n",
      "Train Epoch: 10 [4800/9453 (51%)]\tLoss: 55.000000\tCCC: 0.0000\n",
      "Train Epoch: 10 [5600/9453 (59%)]\tLoss: 49.250000\tCCC: 0.0000\n",
      "Train Epoch: 10 [6400/9453 (68%)]\tLoss: 44.189545\tCCC: 0.0000\n",
      "Train Epoch: 10 [7200/9453 (76%)]\tLoss: 54.000000\tCCC: 0.0000\n",
      "Train Epoch: 10 [8000/9453 (85%)]\tLoss: 44.217400\tCCC: 0.0000\n",
      "Train Epoch: 10 [8800/9453 (93%)]\tLoss: 46.442131\tCCC: 0.0000\n",
      "\n",
      "Test set: Average loss: 35.3931, CCC: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10 + 1):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            train(epoch, dataloders[phase])\n",
    "        if phase == 'val':\n",
    "            test(dataloders[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
